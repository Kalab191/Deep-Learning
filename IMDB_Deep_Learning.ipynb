{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf0754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86ced81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import imdb dataset\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8cc7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of 69664113be75683a8fe16e3ed0ab59fda8886cb3cd7ada244f7d9544e4676b9f so we will re-download the data.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 15s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the data and get the 10000 most common words \n",
    "max_features = 10000\n",
    "maxlen = 20\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37a8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequence \n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42033a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary layers\n",
    "\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "645ecc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(Embedding(10000, 8 , input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56c2b38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b513770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 4s 4ms/step - loss: 0.6674 - acc: 0.6235 - val_loss: 0.6109 - val_acc: 0.7062\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.5328 - acc: 0.7541 - val_loss: 0.5188 - val_acc: 0.7376\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.4577 - acc: 0.7882 - val_loss: 0.4975 - val_acc: 0.7484\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.4224 - acc: 0.8109 - val_loss: 0.4923 - val_acc: 0.7536\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3983 - acc: 0.8228 - val_loss: 0.4945 - val_acc: 0.7540\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3782 - acc: 0.8324 - val_loss: 0.4951 - val_acc: 0.7554\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3593 - acc: 0.8436 - val_loss: 0.4987 - val_acc: 0.7554\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3422 - acc: 0.8530 - val_loss: 0.5049 - val_acc: 0.7526\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3253 - acc: 0.8622 - val_loss: 0.5098 - val_acc: 0.7534\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3083 - acc: 0.8719 - val_loss: 0.5167 - val_acc: 0.7526\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd646a15",
   "metadata": {},
   "source": [
    "## For RAW IMDB TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6402b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cc891b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the directory\n",
    "imdb_dir = 'C:\\\\Users\\\\User\\\\Desktop\\\\Python\\\\pyton t\\\\Deep Learning\\\\imdbraw\\\\aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "labels = []  # Stores the labels of each text as 0 or 1\n",
    "texts = []   # stores the raw text\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname), encoding='utf8')\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89f83c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72633 unique tokens.\n",
      "Shape of data tensor: (17243, 100)\n",
      "Shape of label tensor: (17243,)\n"
     ]
    }
   ],
   "source": [
    "# Turns the text into padded sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "#Splits the data into training and validation data\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "824a1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = models.Sequential()\n",
    "fmodel.add(Dense(32, activation='relu', input_shape=(100,)))\n",
    "fmodel.add(Dense(64, activation='relu'))\n",
    "fmodel.add(Dense(64, activation='relu'))\n",
    "fmodel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "fmodel.compile(optimizer='rmsprop', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18123d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhistory = fmodel.fit(x_train, y_train, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
