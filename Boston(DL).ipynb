{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e75eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Boston Dataset\n",
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d726e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 1us/step\n",
      "65536/57026 [==================================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Loading the data into Training and test set\n",
    "\n",
    "(train_data,train_targets),(test_data,test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b34f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2efbcd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" To prepare the data\n",
    "We use feature-wise normalization: for each feature in the input\n",
    "data (a column in the input data matrix), you subtract the mean of the \n",
    "feature and divide by the standard deviation, so that the feature is centered \n",
    "around 0 and has a unit standard deviation.\n",
    "\"\"\"\n",
    "# Normalizing the data\n",
    "\n",
    "mean = np.mean(train_data,axis=0)\n",
    "std = np.std(train_data,axis=0)\n",
    "\n",
    "train_data -=mean\n",
    "train_data /=std\n",
    "\n",
    "test_data -=mean\n",
    "test_data /=std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69cbbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78c53809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building my model\n",
    "# This function when called will train a deep neural network\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', \n",
    "                           input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e56026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing # 0\n",
      "processing # 1\n",
      "processing # 2\n",
      "processing # 3\n"
     ]
    }
   ],
   "source": [
    "# Using K-fold Validation because of the small amount of dataset available\n",
    "\n",
    "K = 4  # Number of validation\n",
    "num_epoch = 100    # The number of epoch\n",
    "num_val_sample = len(train_data)// K   # Number of validation sample\n",
    "all_scoress =[]                 # This stores the scores for each fold\n",
    "for i in range(K):\n",
    "    print('processing #', i)\n",
    "    val_data = train_data[i * num_val_sample: (i + 1) * num_val_sample]\n",
    "    val_target = train_targets[i * num_val_sample : (i + 1) * num_val_sample]\n",
    "    \n",
    "    partial_train_data = np.concatenate([train_data[:i * num_val_sample],\n",
    "                                       train_data[(i + 1) * num_val_sample:]],\n",
    "                                        axis=0)\n",
    "    partial_train_target = np.concatenate([train_targets[:i * num_val_sample],\n",
    "                                       train_targets[(i + 1) * num_val_sample:]],\n",
    "                                         axis=0)\n",
    "    \n",
    "    # We call the build model function on the training data\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data,partial_train_target,epochs=num_epoch,\n",
    "             batch_size=1, verbose=0)\n",
    "    val_mse , val_mae = model.evaluate(val_data,val_target,verbose=0)\n",
    "    all_scoress.append(val_mae) # appends the validation mean average error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "177e511c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7202330231666565"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This displays the mean score of the K-fold\n",
    "np.mean(all_scoress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b383849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c522847",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c4876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2bdabd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faddab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
